{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Construction using Black-Litterman Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset classes selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index | Name                                  | Ticker | Variable     | Factor            |\n",
    "|-------|---------------------------------------|--------|--------------|-------------------|\n",
    "| 0     | SPDR S&P 500 ETF Trust                | SPY    | sp500        | Benchmark         |\n",
    "| 1     | Berkshire Hathaway Inc.               | BRK.B  | eq_brkb      | Equity Value      |\n",
    "| 2     | UnitedHealth Group Incorporated       | UNH    | eq_unh       | Equity Value      |\n",
    "| 3     | JPMorgan Chase & Co.                  | JPM    | eq_jpm       | Equity Value      |\n",
    "| 4     | Exxon Mobil Corporation               | XOM    | eq_xom       | Equity Value      |\n",
    "| 5     | Broadcom Inc.                         | AVGO   | eq_avgo      | Equity Value      |\n",
    "| 6     | Meta Platforms, Inc.                  | META   | eq_meta      | Equity Growth     |\n",
    "| 7     | Tesla, Inc.                           | TSLA   | eq_tsla      | Equity Growth     |\n",
    "| 8     | Eli Lilly and Company                 | LLY    | eq_lly       | Equity Growth     |\n",
    "| 9     | Visa Inc.                             | V      | eq_visa      | Equity Growth     |\n",
    "| 10    | Mastercard Incorporated               | MA     | eq_mcard     | Equity Growth     |\n",
    "| 11    | Microsoft Corporation                 | MSFT   | eq_msft      | Equity Large Cap  |\n",
    "| 12    | Apple Inc.                            | AAPL   | eq_aapl      | Equity Large Cap  |\n",
    "| 13    | Amazon.com, Inc.                      | AMZN   | eq_amzn      | Equity Large Cap  |\n",
    "| 14    | NVIDIA Corporation                    | NVDA   | eq_nvda      | Equity Large Cap  |\n",
    "| 15    | Alphabet Inc.                         | GOOGL  | eq_googl     | Equity Large Cap  |\n",
    "| 16    | Targa Resources Corp.                 | TRGP   | eq_trgp      | Equity Low Cap    |\n",
    "| 17    | PTC Inc.                              | PTC    | eq_ptc       | Equity Low Cap    |\n",
    "| 18    | Deckers Outdoor Corporation           | DECK   | eq_deck      | Equity Low Cap    |\n",
    "| 19    | Atmos Energy Corporation              | ATO    | eq_ato       | Equity Low Cap    |\n",
    "| 20    | Builders FirstSource, Inc.            | BLDR   | eq_bldr      | Equity Low Cap    |\n",
    "| 21    | iShares Short Treasury Bond ETF       | SHV    | bond_short   | Shorterm Bond     |\n",
    "| 22    | iShares 20+ Year Treasury Bond ETF    | TLT    | bond_long    | Longterm Bond     |\n",
    "| 23    | United States Oil Fund LP             | USO    | com_oil      | Commodity         |\n",
    "| 24    | iShares Gold Trust                    | IAU    | com_gold     | Commodity         |\n",
    "| 25    | iShares MSCI USA Momentum Factor ETF  | MTUM   | momentum     | Momentum Factor   |\n",
    "| 26    | Risk-free rate - Kenneth French Data  | N/A    | rf_rate      | Risk free rate    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import pandas_datareader as pdr\n",
    "import statsmodels.api as sm\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from numpy.linalg import multi_dot\n",
    "import scipy.optimize as sco\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Format for vector output\n",
    "float_formatter = \"{:.6f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the tickers and variable for the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    'SPY', 'BRK-B', 'UNH', 'JPM', 'XOM', 'AVGO', 'META', 'TSLA', 'LLY', 'V',\n",
    "    'MA', 'MSFT', 'AAPL', 'AMZN', 'NVDA', 'GOOGL', 'TRGP', 'PTC', 'DECK',\n",
    "    'ATO', 'BLDR', 'SHV', 'TLT', 'USO', 'IAU', 'MTUM'\n",
    "]\n",
    "variables = [\n",
    "    \"sp500\",\"eq_brkb\", \"eq_unh\", \"eq_jpm\", \"eq_xom\", \"eq_avgo\", \"eq_meta\", \"eq_tsla\", \n",
    "    \"eq_lly\", \"eq_visa\", \"eq_mcard\", \"eq_msft\", \"eq_aapl\", \"eq_amzn\", \"eq_nvda\", \n",
    "    \"eq_googl\", \"eq_trgp\", \"eq_ptc\", \"eq_deck\", \"eq_ato\", \"eq_bldr\", \"bond_short\", \n",
    "    \"bond_long\", \"com_oil\", \"com_gold\", \"momentum\"\n",
    "]\n",
    "StartDate = '2020-04-01'\n",
    "EndDate = '2023-08-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data and save it to the `..\\data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_data = yf.download(tickers = tickers, start=StartDate, end=EndDate, interval='1d')\n",
    "downloaded_data['Adj Close'].to_csv('../data/portfolio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pd.read_csv('../data/portfolio.csv').set_index('Date')\n",
    "daily_price = loaded_data\n",
    "daily_price.columns = variables\n",
    "sp500 = daily_price.sp500\n",
    "daily_price.drop('sp500', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_rets = daily_price.pct_change()[1:]\n",
    "daily_rets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10 , 5))\n",
    "sns.heatmap(daily_rets.corr(), annot=False, vmax=1, vmin=-1, center=0, cmap='vlag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `high_corr` filter to identify highly correlated assets and drop them from the portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_corr(data, threshold=0.5):\n",
    "    col_corr = set()\n",
    "    corr_matrix = data\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_high_corr = high_corr(daily_rets.corr())\n",
    "mask_high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_rets_filtered = daily_rets.drop(mask_high_corr , axis=1)\n",
    "daily_rets_filtered['sp500'] = sp500.pct_change()\n",
    "daily_rets_filtered.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate cummulative returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_rets_filtered = (1 + daily_rets_filtered).cumprod() - 1\n",
    "cum_rets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the daily returns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index, moving `date` as column\n",
    "daily_rets_filtered = daily_rets_filtered.reset_index()\n",
    "# use `melt`\n",
    "df1 = daily_rets_filtered.melt(id_vars=['Date'], var_name='ticker', value_name='daily_return')\n",
    "# add one more column, showing the daily_return as percent\n",
    "df1['daily_return_pct'] = df1['daily_return'] * 100\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df1, x='Date',\n",
    "              y='daily_return_pct', color='ticker',\n",
    "              title='Performance - Daily Simple Returns',\n",
    "              labels={'daily_return_pct':'daily returns (%)'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cummulative daily returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index, moving `date` as column\n",
    "cum_rets_filtered = cum_rets_filtered.reset_index()\n",
    "df2 = cum_rets_filtered.melt(id_vars=['Date'], var_name='ticker', value_name='cum_return')\n",
    "df2['cum_return_pct'] = df2['cum_return'] * 100\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df2, x='Date',\n",
    "              y='cum_return_pct', color='ticker',\n",
    "              title='Performance - Daily Cumulative Returns',   \n",
    "              labels={'cum_return_pct':'Daily cumulative returns (%)', })\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ledoit-Wolf Covariance Shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the shrinkage estimator to estimate the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_rets.drop(mask_high_corr , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_shrink = LedoitWolf().fit(daily_rets.drop(mask_high_corr , axis=1))\n",
    "cov_matrix = cov_shrink.covariance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate market cap weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cap = {'BRK-B':791244000000,'UNH':482258000000, 'XOM':399477000000, 'LLY':610328000000, 'MA':402400000000, 'PTC':20563000000, 'ATO':17465000000, 'USO':1470000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_weight_cal(data):\n",
    "    sum = 0\n",
    "    for value in data.values():\n",
    "        sum += value\n",
    "    for key in data.keys():\n",
    "        data[key] = data[key]/sum\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_weight = market_weight_cal(market_cap)\n",
    "market_weight = np.fromiter(market_weight.values(), dtype=float)\n",
    "market_weight.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Fama French data for risk free rate and the market factor terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famafrench_model = pdr.famafrench.FamaFrenchReader('F-F_Research_Data_Factors_daily',start=StartDate, end=EndDate).read()[0]\n",
    "famafrench_model = famafrench_model[1:]\n",
    "famafrench_model = famafrench_model/100\n",
    "famafrench_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate market risk aversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lambda = \\frac{\\mathbb{E}(r) - r_f}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = famafrench_model.RF\n",
    "market_rets = sp500.pct_change().dropna()\n",
    "market_risk_aversion = (market_rets.mean() - risk_free_rate.mean())/(market_rets.std()**2)\n",
    "market_risk_aversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(market_rets.mean())*252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate market implied return vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    " \\Pi = \\lambda \\Sigma w_{\\text{mkt}} \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_implied_rets = market_risk_aversion * np.matmul(cov_matrix, market_weight)\n",
    "market_implied_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical mean returns\n",
    "observed_exess_returns = daily_rets.drop(mask_high_corr , axis=1).mean()\n",
    "observed_exess_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate weight allocation using the sample mean returns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    " w = (\\lambda \\Sigma)^{-1} \\mu \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_weight = np.matmul(np.linalg.inv(market_risk_aversion * cov_matrix), observed_exess_returns)\n",
    "historical_weight = historical_weight.reshape(-1,1)\n",
    "historical_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Variance optimization approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\underset{w}{\\text {argmax}}\\: & \\mu^T w - \\frac{1}{2} \\lambda w^T\\Sigma w\\\\\n",
    "    \\text{s.t. } \\: & {\\bf 1}^T w = 1, \\quad w \\geq 0 \n",
    "\\end{split}\n",
    "\\end{equation*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scipy\n",
    "import cvxpy as cp\n",
    "\n",
    "# Number of assets\n",
    "n_assets = 8\n",
    "\n",
    "# Define the optimization variables\n",
    "mvo_weight = cp.Variable(n_assets)\n",
    "risk_aversion = market_risk_aversion\n",
    "\n",
    "# Define constraints\n",
    "mvo_constraints = [\n",
    "    cp.sum(mvo_weight) == 1,\n",
    "    mvo_weight >= 0\n",
    "]\n",
    "\n",
    "# Define the portfolio\n",
    "mvo_ret = np.array(observed_exess_returns).reshape(-1,1).T @ mvo_weight\n",
    "mvo_vol = cp.quad_form(mvo_weight, cov_matrix)\n",
    "\n",
    "# Define the problem\n",
    "mvo_problem = cp.Problem(cp.Maximize(mvo_ret - mvo_vol * risk_aversion / 2 ), mvo_constraints)\n",
    "\n",
    "# Solve the problem\n",
    "mvo_problem.solve()\n",
    "\n",
    "# Get the optimal weights\n",
    "mvo_optimal_weights = mvo_weight.value\n",
    "\n",
    "# Print the optimal weights\n",
    "print(\"Optimal Weights:\")\n",
    "print(mvo_optimal_weights.reshape(-1,1).round(4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to compute the optimal weight for different risk aversion values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_variance_optimization(return_vector, cov_matrix, risk_aversion):\n",
    "    # Convert input to NumPy array\n",
    "    if isinstance(return_vector, (pd.Series, pd.DataFrame)):\n",
    "        return_vector = return_vector.values\n",
    "    elif not isinstance(return_vector, np.ndarray):\n",
    "        raise ValueError(\"Input return_vector must be a NumPy array, Pandas Series, or Pandas DataFrame\")\n",
    "\n",
    "    n_assets = len(return_vector)\n",
    "\n",
    "    # Define the optimization variables\n",
    "    mvo_weight = cp.Variable(n_assets)\n",
    "\n",
    "    # Define constraints\n",
    "    mvo_constraints = [\n",
    "        cp.sum(mvo_weight) == 1,\n",
    "        mvo_weight >= 0\n",
    "    ]\n",
    "\n",
    "    # Define the portfolio\n",
    "    mvo_ret = np.array(return_vector).reshape(-1,1).T @ mvo_weight\n",
    "    mvo_vol = cp.quad_form(mvo_weight, cov_matrix)\n",
    "\n",
    "    # Define the objective function\n",
    "    mvo_objective = cp.Maximize(mvo_ret - risk_aversion * mvo_vol / 2)\n",
    "\n",
    "    # Define the problem\n",
    "    mvo_problem = cp.Problem(mvo_objective, mvo_constraints)\n",
    "\n",
    "    # Solve the problem\n",
    "    mvo_problem.solve()\n",
    "\n",
    "    # Get the optimal weights\n",
    "    mvo_optimal_weights = mvo_weight.value\n",
    "\n",
    "    return mvo_optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_variance_optimization(observed_exess_returns, cov_matrix, market_risk_aversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute portfolio annual return\n",
    "observed_exess_returns*252 @ historical_weight * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute portfolio risk\n",
    "historical_weight.T @ cov_matrix *252 @ historical_weight * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximize Sharpe Ratio portfolio approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\underset{w}{\\text {argmax}} \\; &\\frac{\\mu^\\top w - r_f }{\\sqrt{w^\\top \\Sigma w}} \\\\\\\\\n",
    "        \\text{s.t.} \\quad   &w^{\\top} \\mathbf{1} = 1 \\\\\n",
    "                            &w \\geq 0.    \n",
    "    \\end{split} \n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Statistics\n",
    "def portfolio_stats(weights):\n",
    "    weights = np.array(weights)\n",
    "    port_rets = weights.T @ np.array(observed_exess_returns) - risk_free_rate.mean()\n",
    "    port_vols = np.sqrt(multi_dot([weights.T, cov_matrix, weights]))\n",
    "    return np.array(port_rets/port_vols)\n",
    "# Maximizing sharpe ratio\n",
    "def max_sharpe_ratio(weights):\n",
    "    return -portfolio_stats(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify constraints, bounds and initial weights\n",
    "n_assets = len(observed_exess_returns)\n",
    "cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "bnds = tuple((0,1) for x in range(n_assets))\n",
    "initial_wts = n_assets*[1./n_assets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing for maximum sharpe ratio\n",
    "opt_sharpe = sco.minimize(max_sharpe_ratio, initial_wts, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "msr_optimal_weights = opt_sharpe.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_optimal_weights.reshape(-1,1)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BL Implememt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1.0\n",
    "P = np.array([[1,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,-1]])\n",
    "Q = np.array([0.2/252, 0.005/252]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.DataFrame(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Omega matrix\n",
    "Omega = np.matmul(np.matmul(P, cov_matrix), P.T)\n",
    "Omega = np.diag(np.diag(Omega))\n",
    "Omega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(r) = \\mu_\\text{BL} = \\left[ (\\tau\\Sigma)^{-1} + P^\\top \\Omega^{-1} P \\right]^{-1} \\left[ (\\tau\\Sigma)^{-1} \\Pi + P^\\top \\Omega^{-1} Q \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the first half of the equation\n",
    "tauSigmainv = np.linalg.inv(tau*cov_matrix) \n",
    "first_half = np.linalg.inv(tauSigmainv +  P.T @ np.linalg.inv(Omega) @ P)\n",
    "\n",
    "# Compute the second half of the equation\n",
    "second_half = (tauSigmainv @ market_implied_rets).reshape(-1,1) + P.T @ np.linalg.inv(Omega) @ Q\n",
    "\n",
    "# New combined return vector\n",
    "combined_return_vector = first_half @ second_half\n",
    "\n",
    "combined_return_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute updated weight\n",
    "blm_weights = np.matmul(np.linalg.inv(market_risk_aversion * cov_matrix), combined_return_vector).flatten()\n",
    "blm_weights.round(4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Black-Litterman class to compute the weight with specified inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackLittermanModel:\n",
    "    def __init__(self, market_weight, tau, P, Q, cov_matrix):\n",
    "        self.market_weight = market_weight\n",
    "        self.risk_aversion = 4.10\n",
    "        self.tau = tau\n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self._market_risk_aversion = 4.10\n",
    "\n",
    "    def combined_return_vector(self):\n",
    "        # Convert market_weight to row vector\n",
    "        market_weight = self.market_weight.reshape(1, -1)\n",
    "\n",
    "        # Compute market implied return vector\n",
    "        market_implied_rets = self._market_risk_aversion * np.matmul(self.cov_matrix, market_weight.T)\n",
    "\n",
    "        # Compute Omega matrix\n",
    "        Omega = np.diag(np.diag(np.matmul(np.matmul(self.P, self.cov_matrix), self.P.T)))\n",
    "\n",
    "        # Compute the first half of the equation\n",
    "        tauSigmainv = np.linalg.inv(self.tau * self.cov_matrix)\n",
    "        first_half = np.linalg.inv(tauSigmainv + self.P.T @ np.linalg.inv(Omega) @ self.P)\n",
    "\n",
    "        # Compute the second half of the equation\n",
    "        second_half = (tauSigmainv @ market_implied_rets).reshape(-1, 1) + self.P.T @ np.linalg.inv(Omega) @ self.Q\n",
    "\n",
    "        # New combined return vector\n",
    "        combined_return_vector = np.matmul(first_half, second_half)\n",
    "\n",
    "        return combined_return_vector.reshape(-1, 1)\n",
    "\n",
    "    def blm_weight(self):\n",
    "        combined_return_vector = self.combined_return_vector()\n",
    "\n",
    "        # Compute Black Litterman weight\n",
    "        blm_weight = np.matmul(np.linalg.inv(self.risk_aversion * self.cov_matrix), combined_return_vector)\n",
    "\n",
    "        return blm_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Risk Aversion Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 different risk aversion level\n",
    "risk_seeking = 1\n",
    "risk_neutral = market_risk_aversion\n",
    "risk_averse = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight allocation for risk seeking investors\n",
    "mean_variance_optimization(observed_exess_returns, cov_matrix, risk_seeking).round(4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight allocation for risk neutral investors\n",
    "mean_variance_optimization(observed_exess_returns, cov_matrix, risk_neutral).round(4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight allocation for risk averse investors\n",
    "mean_variance_optimization(observed_exess_returns, cov_matrix, risk_averse).round(4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolios Returns and Risks Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_analysis(weights, annual_rets, cov_matrix, rf_rate):\n",
    "    weights = weights.reshape(-1,1)\n",
    "    # Compute portfolio annual return\n",
    "    ret = weights.T @ annual_rets\n",
    "    ret = ret[0]\n",
    "    # Compute portfolio risk\n",
    "    risk = np.sqrt(multi_dot([weights.T, cov_matrix, weights]))\n",
    "    risk = risk[0][0]\n",
    "    # Compute portfolio Sharpe ratio\n",
    "    sharpe_ratio = (ret - rf_rate)/risk\n",
    "    return [ret, risk, sharpe_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define annual params\n",
    "annual_rets = observed_exess_returns*252\n",
    "annual_cov = cov_matrix*252\n",
    "annual_rf = risk_free_rate.mean()*252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the portfolio return, risk and Sharpe ratio\n",
    "max_sharpe_portfolio = portfolio_analysis(msr_optimal_weights, annual_rets, annual_cov, annual_rf)\n",
    "blm_portfolio = portfolio_analysis(blm_weights, annual_rets, annual_cov, annual_rf)\n",
    "max_sharpe_portfolio, blm_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the benchmark historical return and risk\n",
    "benchmark_rets = sp500.pct_change().dropna().mean()*252\n",
    "benchmark_risk = sp500.pct_change().dropna().std()*np.sqrt(252)\n",
    "benchmark_sr = (benchmark_rets - annual_rf)/benchmark_risk\n",
    "benchmark_rets.round(4)*100, benchmark_risk.round(4)*100, benchmark_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cummulative portfolio return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame of return data to plot\n",
    "portfolio_ret = pd.DataFrame()\n",
    "portfolio_ret['sp500'] = sp500.pct_change().dropna()\n",
    "portfolio_ret['MSR'] = [daily_rets_filtered.iloc[i][1:9] @ msr_optimal_weights for i in range(len(daily_rets_filtered))]\n",
    "portfolio_ret['BLM'] = [daily_rets_filtered.iloc[i][1:9] @ blm_weights for i in range(len(daily_rets_filtered))]\n",
    "portfolio_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cummulative returns\n",
    "portfolio_cum_ret = (1 + portfolio_ret).cumprod() - 1\n",
    "# reset the index, moving `date` as column\n",
    "portfolio_cum_ret = portfolio_cum_ret.reset_index()\n",
    "portfolio_cum_ret = portfolio_cum_ret.melt(id_vars=['Date'], var_name='symbol', value_name='cum_return')\n",
    "portfolio_cum_ret['cum_return_pct'] = portfolio_cum_ret['cum_return'] * 100\n",
    "portfolio_cum_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(portfolio_cum_ret, x='Date',\n",
    "              y='cum_return_pct', color='symbol',\n",
    "              title='Portfolios - Daily Cumulative Returns',   \n",
    "              labels={'cum_return_pct':'Daily cumulative returns (%)', })\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
